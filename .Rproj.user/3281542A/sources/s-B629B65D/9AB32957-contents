---
title: "Infections of the body and the mind"
output: html_document
css:    mondstyle.css
editor_options: 
chunk_output_type: inline

---

**by [Julio Amador Diaz Lopez](https://www.imperial.ac.uk/people/j.amador)^[Imperial College Business School] and [Ralf Martin](https://www.imperial.ac.uk/people/r.martin)^[Imperial College Business School  & Centre for Economic Perforamnce, LSE]** 


<!--html_preserve-->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-3928947-4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-3928947-4');
  </script>
<!--/html_preserve-->


Last update:  `r format(Sys.time(), '%B %d , %Y - %H:%M ')`

```{r Notes,eval=FALSE,include=FALSE}
#We are using this: https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/use-twitter-api-r/

#This is useful too: https://rtweet.info/

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rtweet)
library(tidytext)

# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
library(gdata)



source("../code/gettoken.R")


```





```{r load ts data,include=FALSE}
#`r ntweets'

scomb=readRDS("../results/scomb.rda")

#adf=readRDS("../results/adf.rda")

#scomb=scomb%>%merge(adf,by="date")
#scomb=scomb%>%mutate(scaler=ifelse(scaler<1,1,scaler))
#scomb=scomb%>%mutate(stweets=tweets*scaler) %>%mutate(hoaxsh=igtweets/tweets)

ntweets= round(sum(scomb$tweets) /10^6,2)
nhoax=sum(scomb$igtweets)

# save to github
write.csv(scomb,"./data/hoaxshare_over_time.csv")

```



# Introduction


The coronavirus pandemic has changed everything overnight. Unfortunately, as the genetic sequence of the virus started to make its deadly journey through bodies around the world, in parallel a memetic sequence emerged in the minds of some people: the idea that covid19 pandemic is not real and a hoax. Indeed, the worry is that the two infections exist in a symbiotic relationship with one helping to advance the survival and spread of the other. Here we report on our ongoing efforts to map the spread of memetic infection using Twitter. Since March 23 we have been sampling tweets mentioning the terms “corona” and/or “covid”. Currently, we have collected   `r ntweets` million  tweets.


Some emerging results include the following:

- There is no sign that hoax meme has run its course. Over our admittedly short sample period of about two weeks the overal level of hoaxism remained stable.
- There is some evidence that Donald Trump has to answer for the hoax meme. Hoax believers are particularly obsessed with him.
- There is evidence that believe in Hoax led to more higher covid cases than necessary. Across US states we find a strong correlation between hoax believer rates and covid cases. 
- We have evidence that is consistent with the idea that interaction between population density and hoaxism is a major factor in explaining the severety of the pandemic.
- We have to be careful with causal interpretations but the numbers would imply that US covid cases could be about 20% lower in absence of hoaxism.




# Hoaxism over time

How bad is the hoax infection and is it getting better or worse? To identify tweeters believing in the hoax (or promoting the hoax idea) we look for tweets with one of the following hastags:

- "#hoax"
- "#coronahoax"
- "#covidhoax"


Using hashtags instead of string searches of the same terms provides a good distinction between tweets who display support for hoaxsim vs tweets criticising hoaxism. Note that this is likely a conservative way of counting hoaxist tweets and in reality a larger fraction of tweets are from people supporting hoaxist ideas.

Below is a time series plot of the share of hoaxist tweets over our sample period.^[Data is [here](https://mondpanther.github.io/economemics.github.io/data/hoaxshare_over_time.csv)]]
 we report separate series for the Us and UK. Assigning location to tweets is notoriously difficult as most users have switch off detailed location tracking. In the figure below we base location on the analysis of a free text field where users can write something about their whereabouts. In many cases this refers to known areas although the detail varies (e.g. London, UK vs the Universe). Often it also involves phantasy locations (e.g. Walhalla). Hence, our "other" category might include tweeters from either the UK or Us who have chosen not to reveal their location.

Note that towards the begining of the sample period the share of hoax tweets in all covid related tweets is less than 0.5%. However, the weekend  around the 28th of March saw a major outbreak of Hoaxism that was particularly bad in the UK. This has subsided somewhat come March 30. The whole sample trend would suggest that hoaxism is fairly stable and not subsiding, although there seems to be a declining trend for the last couple of days.

```{r tsplot,echo=FALSE}

scomb=scomb%>% group_by(country) %>% arrange(date) %>% mutate(cumtweets=cumsum(tweets),cumigtweets=cumsum(igtweets)) %>% mutate(cumhoaxsh=cumigtweets/cumtweets)


tsplot=ggplot(scomb, aes(x = date,y=hoaxsh,color=country  )  )+geom_point() +  geom_line() + theme_minimal() + xlab("Time") +ylab("Share of hoax tweets [%]")

tsplot
```



```{r, echo=FALSE}
scomb=scomb%>%group_by(date) %>% summarise(tot=sum(tweets)) %>% merge(scomb,by="date") %>% mutate(totsh=tweets/tot *100)

  tsp2=ggplot(scomb %>% filter(country!="other"), aes(x = date,y=igtweets,color=country  )  )+geom_point() +  geom_line() + theme_minimal() + xlab("Time") +ylab("Share in %")
  
  #tsp2
```










# Why hoaxism?

What are drivers of hoaxism? We can start exploring this by looking at the tweets of hoaxists more widely. Below we plot a word cloud of the last 1000 tweets of the 300 most prolific hoaxists. One hypothesis is that hoaxism has been fueled by Trumpism. Because of worries that a strong response to the pandemic could negatively affect the economy and thereby his re-election chances, he had a vested interest in playing down the crisis. The word cloud confirms that obsession with trump is prevalent among hoxists.

```{r wordclouds,include=FALSE, echo=FALSE}

#tmls['ccid'] <-     mapply(grepl, pattern="climate", x=tolower(tmls$text) )
#tmls['trumpid'] <-  mapply(grepl, pattern="trump", x=tolower(tmls$text) )
#tmls['clintonid'] <-  mapply(grepl, pattern="clinton", x=tolower(tmls$text) )  
#tmls['coronaORcovid'] <-     mapply(grepl, pattern="corona", x=tolower(tmls$text) ) |  mapply(grepl, pattern="covid", x=tolower(tmls$text) )
#summary(tmls$trumpid)
#summary(tmls$clintonid)
#summary(tmls$coronaORcovid)
#cc=tmls%>%filter(ccid==TRUE)
#nrow(tmls%>%filter(ccid==TRUE))

library(tm)



tmls=readRDS("../results/tmls.rda")
tmlsnon=readRDS("../results/tmlsnon.rda")

#names(tmls)

keeps=c('status_id','text','created_at')
texts=tmls[,keeps]


texts=texts%>%dplyr::rename(doc_id=status_id)


textsref=tmlsnon[,keeps]
textsref=textsref%>%dplyr::rename(doc_id=status_id)






#install.packages("wordcloud")
library(wordcloud)
#install.packages("RColorBrewer")
library(RColorBrewer)
#install.packages("wordcloud2")
library(wordcloud2)


wc=function(texts){
    texts$text=gsub(" re ", " ", texts$text)
    texts$text=gsub("untuk", " ", texts$text)
    texts$text=gsub("uuuuuu", " ", texts$text)
    texts$text=gsub("uuuuu", " ", texts$text)
    texts$text=gsub("uuuu", " ", texts$text)
    texts$text=gsub("uuu", " ", texts$text)
    texts$text=gsub("uue", " ", texts$text)
    texts$text=gsub("uu", " ", texts$text)
    texts$text=gsub("más", "", texts$text)
    texts$text=gsub("https\\S*", "", texts$text)
    texts$text=gsub("@\\S*", ""    , texts$text)
    texts$text=gsub("amp", "", texts$text) 
    texts$text=gsub("[\r\n]", "", texts$text)
    texts$text=gsub("[[:punct:]]", "", texts$text)
  
    
    
    docs=Corpus(DataframeSource(texts))
    docs <- docs %>%
      tm_map(removeNumbers) %>%
      tm_map(removePunctuation) %>%
      tm_map(stripWhitespace)
    
    docs <- tm_map(docs, content_transformer(tolower))
    docs <- tm_map(docs, removeWords, stopwords("english"))
    
    dtm <- TermDocumentMatrix(docs) 
    matrix <- as.matrix(dtm) 
    words <- sort(rowSums(matrix),decreasing=TRUE) 
    df <- data.frame(word = names(words),freq=words)
    
    
    set.seed(1234) # for reproducibility 
    wordcloud(words = df$word, freq = df$freq, min.freq = 1,       max.words=100, random.order=FALSE, rot.per=0.2,                colors=brewer.pal(8, "Dark2"))
}
```


```{r wordcloudsregs, cache=TRUE, echo=FALSE}

wc(texts)
#names(combstream)
#textsref=subset(comb, select = c(status_id,text))
#textsref=textsref%>%rename(doc_id=status_id)
```




```{r,cache=TRUE, echo=FALSE}

texts['hoaxer']=TRUE
textsref['hoaxer']=FALSE


ctexts=rbind(texts,textsref)



library(lubridate)

start.date = ymd_hms("2010-01-01 00:00:00")
end.date   = as_datetime(now()) #ymd_hms("2020-04-02 01:00:00")
#end.date   = ymd_hms("2020-04-04 01:00:00")

breaks = seq(start.date, end.date, "1 week")



ctexts['weeks'] = cut(ctexts$created_at, breaks=breaks)

ctexts['trumpid'] <-  mapply(grepl, pattern="trump", x=tolower(ctexts$text) )



ctexts['climateid'] <-  mapply(grepl, pattern="climate", x=tolower(ctexts$text) )
ctexts['hoaxid']    <-  mapply(grepl, pattern="hoax", x=tolower(ctexts$text) )

ctexts=ctexts%>%mutate(climateXhoaxid=climateid==TRUE & hoaxid==TRUE)





tr2=lm(climateid~hoaxer,ctexts)
tr3=lm(climateXhoaxid~hoaxer,ctexts)

tr=lm(trumpid~hoaxer,ctexts)
#summary(tr)
#summary(tr2)
#summary(tr3)




```



For comparison, here is a word cloud of the 300 most prolific non-hoaxist covid related tweeters. Trump is relevant here too although do a smaller degree: hoaxers have a  `r round(tr$coefficients[["hoaxerTRUE"]]*100,2) ` percentage point higher probability of mentioning Trump (The share of Trump mentions across both groups is `r round(mean(ctexts$trumpid)*100,2)`%). Of course it might also be that one group is supporting Trump whereas the other is opposing him. We will address this in future work. 




```{r nonhoaxer,cache=TRUE,echo=FALSE}
wc(textsref)


```



Also note that the term "filmyourhospital" shows up prominently, which according [reports](https://www.mediamatters.org/coronavirus-covid-19/coronavirus-denying-conspiracy-theory-hashtag-spreading-tiktok-infowars-host) is a hastag pushed by right-wing commentators.


# The consequences of memetic infections

We examine if US state level hoax infection rates are correlated with reported covid19 infection rates. This is interesting to gauge if mis-information has any effect on actual outcomes. Clearly, from a simple exercise like that we cannot draw overly strong conclusions about causal effects. However, it is a useful starting point. 
The figure below^[Scatterplot data is [here](https://mondpanther.github.io/economemics.github.io/data/US_scatter.csv).] is a scatter plot of state level per capita infection rates on the share of hoax tweets (in percent) from within the state. There is clearly a positive relationship. What is particularly striking is that New York is not only extreme in terms of infections but also in the prevalence of hoaxism.

<br/><br/>


```{r statelevel,echo=FALSE}
#library(lfe)


stats=readRDS("../results/stats.Rda")
statsbyd=readRDS("../results/statsbyd.Rda")





stats=stats%>%mutate(pop=pop/1000, hoaxshXdensity=(hoaxsh)*(density-mean(density)),tweetsPCXdensity=(tweetsPC)*(density-mean(density)))


statsbyd=statsbyd%>%group_by(date)%>%mutate(mtweetsPC=mean(tweetsPC),mhoaxsh=mean(hoaxsh),mdensity=mean(density))

#statsbyd=statsbyd%>%filter(as.character(date)>"2020-03-26")


statsbyd=statsbyd%>%mutate(pop=pop/1000, hoaxshXdensity=(hoaxsh) *(density-mdensity),tweetsPCXdensity=(tweetsPC) * (density-mdensity) )

#statsbyd <- statsbyd %>% arrange(., state, date) %>%
#  group_by(state) %>%
#  mutate(dayx = 1:n())



r1=lm(casesPC~hoaxsh,stats)
r2=lm(casesPC~hoaxsh+density+tweetsPC,stats)


#r3=lm(casesPC~hoaxsh + density + tweetsPC ,stats)
r4 =lm(casesPC~hoaxsh + density + tweetsPC +  hoaxshXdensity  + tweetsPCXdensity  ,stats)
#r5=lm(casesPC~hoaxsh*density+pop+tweetsPC*density,stats%>%filter(state!="New York"))

#lm.cluster(data, formula, cluster, weights=NULL, subset=NULL )

#library(miceadds)
#r6=lm.cluster(data=statsbyd, formula=casesPC~hoaxsh*density+tweetsPC*density+factor(state)+factor(date)  , cluster="state"  )

r6=lm(casesPC~hoaxsh+tweetsPC +  hoaxshXdensity + tweetsPCXdensity   + factor(state)+factor(date), statsbyd )
r7=lm(casesPC~hoaxsh+tweetsPC +  hoaxshXdensity + tweetsPCXdensity   + factor(state)+factor(date), statsbyd %>%filter(state!="New York") )


#r8=lm(deathsPC~casesPC+casesPC:density+ tweetsPC:density+hoaxsh:density+hoaxsh+tweetsPC+factor(state)+factor(date), statsbyd )

#r6=(felm((casesPC)~hoaxsh*density+tweetsPC*density+factor(state)+factor(date)|0|0|state,data=statsbyd  ))
#ols1 <- felm(y ~ x + z|0|0|firmid, data = petersen)
#summary(r4)


#summary(r6)
#summary(r8)
#summary(r6,cluster=c("fstate"))

#summary(lm((casesPC)~hoaxsh:density+tweetsPC:density+hoaxsh+tweetsPC+factor(state)+factor(date), statsbyd  ))


#summary(lm((casesPC)~hoaxsh*density+tweetsPC*density+factor(date), statsbyd  ))


#summary(lm((casesPC)~hoaxsh*density+tweetsPC*density, stats  ))


#summary(lm(casesPC~hoaxsh+density,stats%>%filter(hoaxsh<7 & casesPC<10)))

#summary(lm(casesPC~hoaxsh+density,stats%>%filter(state!="New Jersey" & state!="New York")))

#stats=stats%>%mutate(DD=  (r4$coefficients[["hoaxsh"]]+r4$coefficients[["hoaxsh:density"]] * density ) *hoaxsh * pop/1000)
#stats=stats%>%mutate(DD=  (r3$coefficients[["hoaxsh"]]) *hoaxsh * pop)
#sum(stats$DD)
#sum(stats$cases)


#sum(stats$DD)
#sum(stats$cases)

#summary(r1)
#summary(r2)
#summary(r3)
#summary(r6)

#sss=stats%>%summarise(pop=sum(pop),cases=sum(cases), tweets=sum(tweets),hoax=sum(hoax))
#sss=sss%>% mutate(hoaxsh=hoax/tweets*100,casesPC=cases/pop)
#sss=sss%>% mutate(saved=(r4$coefficients[["hoaxsh"]]+r4$coefficients[["hoaxsh:density"]] * density ) * hoaxsh  *  pop)



#coef=r6$coefficients[["hoaxsh"]]
#r6$coefficients[["hoaxshXdensity"]]


#stats=stats%>% mutate(saved=(r3$coefficients[["hoaxsh"]]) * hoaxsh  *  pop)
#stats=stats%>% mutate(saved=(r5$coefficients[["hoaxsh"]]+r5$coefficients[["hoaxsh:density"]] * density ) * hoaxsh  *  pop)
stats=stats%>% mutate(saved=(r6$coefficients[["hoaxsh"]]+r6$coefficients[["hoaxshXdensity"]] * density ) * hoaxsh  *  pop)



#stats=stats%>% mutate(saved=ifelse(saved>cases, cases,saved))  # remove over shoots

#summary(stats$saved)

#look=stats%>% select(c(hoaxsh,saved,casesPC))
#View(look)


#stats=stats%>%mutate(hoaxsh2=hoaxsh^2,density2=density^2)

saved=base::sum(stats$saved)  
all=base::sum(stats$cases)


#summary(lm(casesPC~hoaxsh+density,stats))
#summary(lm(casesPC~hoaxsh*density+tweetsPC,stats))
#summary(lm(casesPC~hoaxsh*density+tweetsPC+pop,stats))
#summary(lm(casesPC~hoaxsh*density+tweetsPC+pop,stats))


#summary(lm(log(casesPC)~hoaxsh*density+tweetsPC+pop,stats))
#summary(lm(log(casesPC)~hoaxsh*density+tweetsPC*density+pop,stats))
#summary(lm(casesPC~density+tweetsPC,stats))
#summary(lm(casesPC~hoaxsh*density+pop+tweetsPC+density2+hoaxsh2,stats%>%filter(state!="New York")))
#summary(lm(casesPC~hoaxsh+density+pop+tweetsPC,stats%>%filter(state!="New York")))

#summary(lm(casesPC~hoaxsh+density+tweetsPC,stats%>%filter(state!="New York")))


# Write to github repo

# save to github
write.csv(stats,"./data/US_scatter.csv")
write.csv(statsbyd,"./data/US_timeseries.csv")



```

<br/><br/>



```{r,  echo=FALSE}
library(ggrepel)
ggplot(stats) + 
    aes(y=casesPC, x=hoaxsh,label=state) +
    geom_point() +geom_text_repel(cex=2)+
    stat_smooth(method = "lm", se = FALSE)+theme_minimal()+ylab("Covid Cases per 1000")+xlab("Share of hoax tweets in %")


```

<br/><br/>

An alternative explanation for the striking infection rates in New York is the relative density of New York. That's why we also examine the relationship between infection rates and density (in people per square mile). Indeed there is a positive relationship as well. But New York seems to be more of an outlier in terms of density. 
Indeed one potential hypothesis the two figures combined suggests is that there might be an interaction effect between hoaxism and density. Take for instance Alaska, which has the second highest rates of hoaxism, but much lower infection rates than New York. Of course it's also the least densiley populated state. On the other hand: consider New Jersey which is actually more dense than New York but has much lower rates of infection. It turns out that hoaxism is also less prevalent there.



<br/><br/>


```{r,  echo=FALSE}
library(ggrepel)
ggplot(stats) + 
    aes(y=casesPC, x=density,label=state) +
    geom_point() +geom_text_repel(cex=2)+
    stat_smooth(method = "lm", se = FALSE)+theme_minimal()+ylab("Covid Cases per 1000")+xlab("Density [People per square mile]")


```


<br/><br/>


To explore this more below we also undertake regression analysis.^[Underlying data is [here](https://mondpanther.github.io/economemics.github.io/data/US_timeseries.csv).] The Table below shows that:

- Hoaxsim is indeed significantly and positively related to hoaxism (Column 1). The coefficient implies that a 1 percentage point higher hoaxism level is associated with 1.38 extra covid patients per 1000 citizens.

- This is result is highly robust to the inclusion of further controls such as population density, population size and covid tweet intensity (covid related tweets per 1000 people) in column 2. 

- The hoaxism and density interaction hypothesis is confirmed in column 3 where we include the interaction of both variables as an additional regression coefficient (as well as the interaction of covid tweet intensity with density as additional control)

- In column 4, we identify the model from daily data rather than a cross sectional variation of the latest available period (day). This allows us to control to include state as well as day control (density is no longer separately identified as it becomes a fixed state level characterstics). Hence we implicity control for all fixed state characteristics that could might be confounding our estimate. This preserves our qualitative conclusions alhtough the estimates coefficients become lower.

- In column 5 we repeat the exercise while dropping all observations from New York. This has little impact on the findings related to hoaxism.

We have to be cautious with causal claims at this stage. Our results could be contingent on our  simple model specification or crude aggregation (e.g. we don't take into account that New York state consists of the metropolitian area of New York as well as rarther rural parts, although as we saw in column 5, the results are not contingent on New York). Still, to understand if the results are not only statistically significant but also quantitatively meaningful it is useful to ask what - if taken at face value - the impact of haoxism would be. Using the estimates from column 4 which we consider our most reliable at this stage would imply that without hoaxism  we had  `r format(round(saved,0),scientific=FALSE) ` covid cases less (of a total of `r format(all,scientific=FALSE)`), as of `r max(stats$date)`. Clearly, this is substantial.

<br/><br/>


```{r  message=FALSE, results='asis',echo=FALSE} 
library(stargazer)

cns=names(coef(r6)) 
cns=cns[ grepl( "factor", cns)==TRUE]

cns=as.vector(cns)

cns=c("factor(state)Arkansas" ,"Population")

#r6=r6$lm_res
clabels = c("Hoax Tweets Share", "Population density",  "Tweets per capita",    "Hoax X Density" ,"Tweets X Density")

#cns=c("Constant","Population")
stargazer(r1,  r2, r4 ,r6,r7, type = "html",  omit=c("state","date","Constant")  ,
          covariate.labels =clabels,
          df = FALSE,
          dep.var.labels   = "Covid19 Cases per capita",
                  add.lines = list(
        c("States Controls", "No", "No", "No","Yes", "Yes"),
        c("Day Controls", "No", "No", "No","Yes", "Yes"),
        c("Sample", "Last Day", "Last Day", "Last Day","Daily", "NY dropped")
          ))

```
 
<br/><br/>




```{r, eval=FALSE,echo=FALSE}
df=statsbyd %>% filter(state=="New York" | state=="New Jersey" |state=="Florida" | state=="Alaska" | state=="California")
tsplot=ggplot(df , aes(x = date,y=hoaxsh,color=state  )  )+geom_point() +  geom_line() + theme_minimal() + xlab("Time") +ylab("Share of hoax tweets [%]")
tsplot


df=statsbyd %>% filter(state=="New York" | state=="New Jersey" |state=="Florida" | state=="Alaska" | state=="California")
tsplot=ggplot(df , aes(x = date,y=DcasesPC,color=state  )  )+geom_point() +  geom_line() + theme_minimal() + xlab("Time") +ylab("Extra cases PC")
tsplot


df=statsbyd %>% filter(state=="New York" )
tsplot=ggplot(df , aes(x = date,y=casesPC,color=state  )  ) +  geom_line()+geom_line(aes(x = date,y=hoaxsh,color=state  ) ) + theme_minimal() + xlab("Time") +ylab("Extra cases PC")
tsplot


df=statsbyd %>% filter(state=="New York" )
tsplot=ggplot(df , aes(x = date,y=casesPC,color=state  )  ) +  geom_line()+geom_line(aes(x = date,y=tweetsPC,color=state  ) ) + theme_minimal() + xlab("Time") +ylab("Extra cases PC")
tsplot



df=statsbyd %>% filter(state=="Louisiana" )
tsplot=ggplot(df , aes(x = date,y=casesPC,color=state  )  ) +  geom_line()+geom_line(aes(x = date,y=hoaxsh,color="blue"  ) ) + theme_minimal() + xlab("Time") +ylab("Extra cases PC")
tsplot




df=statsbyd %>% filter(state=="Alaska" )
tsplot=ggplot(df , aes(x = date,y=casesPC,color=state  )  ) +  geom_line()+geom_line(aes(x = date,y=hoaxsh,color="blue"  ) ) + theme_minimal() + xlab("Time") +ylab("Extra cases PC")
tsplot

df=statsbyd %>% filter(state=="New Jersey" )
tsplot=ggplot(df , aes(x = date,y=casesPC,color=state  )  ) +  geom_line()+geom_line(aes(x = date,y=hoaxsh,color="blue"  ) ) + theme_minimal() + xlab("Time") +ylab("Extra cases PC")
tsplot




```


